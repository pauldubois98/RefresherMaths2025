\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Primes}{\mathbb{P}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\txtand}{\text{ and }}
\newcommand{\txtor}{\text{ or }}
\newcommand{\lxor}{\veebar}

%opening
\title{Notes on Binary Classification}
\author{DSBA Mathematics Refresher 2025}
\date{}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		
	\end{abstract}
	
	This session will explore different methods for constrained optimization, with a particular focus on the Lagrange multiplier technique.
	
	\section{Unconstrained Optimization}
	
	Unconstrained optimization refers to the problem of optimizing a function without any restrictions or constraints on the variables.
	Mathematically, this can be expressed as:
	$$
	\text{Find } \mathbf{x}^* \text{ such that } f(\mathbf{x}^*) = \min_{\mathbf{x}} f(\mathbf{x})
	$$
	where $f(\mathbf{x})$ is the objective function.
	
	The necessary condition for \(\mathbf{x}^*\) to be an extremum is that the gradient of $f$ at $\mathbf{x}^*$ is zero:
	$$
	\nabla f(\mathbf{x}^*) = \mathbf{0}
	$$
	
	\subsection{Direct Calculation Method}
	The direct method involves taking the derivative of the function with respect to each variable, setting these derivatives equal to zero, and solving the resulting system of equations.
	
	\textbf{Example:}
	\noindent\\
	Consider the function $f(x, y) = x^2 + y^2$. To find the minimum, we calculate:
	$$
	\frac{\partial f}{\partial x} = 2x, \quad \frac{\partial f}{\partial y} = 2y
	$$
	Setting these equal to zero, we get:
	$$
	2x = 0 \quad \text{and} \quad 2y = 0
	$$
	This gives $x = 0$ and $y = 0$, which is the minimum point.
	
	\subsection{Iterative Process}
	In a previous session, we explored iterative process to find the extremum of a function.
	This is often used when solving $\nabla f(\mathbf{x}^*) = \mathbf{0}$ can not be done in reasonable time.
	
	\section{Constrained Optimization}
	In many practical problems, the optimization process is subject to certain constraints.
	These constraints can be either equality or inequality constraints.
	
	\subsection{Equality Constrained Optimization}
	Equality constrained optimization problems can be expressed as:
	$$
	\text{Minimize } f(\mathbf{x})
	\quad
	\text{subject to}
	\quad
	g_i(\mathbf{x}) = 0
	\quad
	\text{for } i = 1, \dots, m
	$$
	
	\subsubsection{Direct Calculation Method}
	For some simple problems, we can solve the constraint explicitly and substitute it back into the objective function to reduce the problem to an unconstrained optimization problem.
	
	\textbf{Example:}
	Minimize $f(x, y) = x^2 + y^2$ subject to the constraint $x + y = 1$.
	\\
	Solve the constraint for $y = 1 - x$, and substitute into $f(x, y)$:
	$$
	f(x, 1-x) = x^2 + (1-x)^2 = 2x^2 - 2x + 1
	$$
	Minimize the resulting function by finding the derivative and setting it to zero:
	$$
	\frac{d}{dx}(2x^2 - 2x + 1) = 4x - 2 = 0
	$$
	This gives $x = \frac{1}{2}$, and hence $y = \frac{1}{2}$.
	
	\subsubsection{Lagrange Multiplier Method}
	The Lagrange multiplier method introduces an auxiliary variable (the Lagrange multiplier) to incorporate the constraints into the objective function.
	
	\textbf{Lagrange Function:}
	$$
	\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \sum_{i=1}^m \lambda_i g_i(\mathbf{x})
	$$
	To find the stationary points, we solve the system:
	$$
	\nabla_\mathbf{x} \mathcal{L}(\mathbf{x}, \lambda) = \mathbf{0}
	$$
	
	\textbf{Example:}
	Minimize $f(x, y) = x^2 + y^2$ subject to $x + y = 1$.

	\textbf{Step 1: Construct the Lagrange Function}\\
	The Lagrangian is:
	$$
	\mathcal{L}(x, y, \lambda) = x^2 + y^2 + \lambda (x + y - 1)
	$$
	
	\textbf{Step 2: Compute the Partial Derivatives}\\
	The partial derivatives of the Lagrangian are:
	$$
	\frac{\partial \mathcal{L}}{\partial x} = 2x + \lambda = 0, \quad \frac{\partial \mathcal{L}}{\partial y} = 2y + \lambda = 0, \quad \frac{\partial \mathcal{L}}{\partial \lambda} = x + y - 1 = 0
	$$
	
	\textbf{Step 3: Solve the System of Equations}\\
	Solving this system gives $x = y = \frac{1}{2}$, $\lambda = -1$, which matches our previous result.
	
	\textbf{Step 4: Determine the Optimal Point}\\
	Substitute $x = \frac{1}{2}$, $y = \frac{1}{2}$ into the original function:
	$$
	f\left(\frac{1}{2}, \frac{1}{2}\right) = \left(\frac{1}{2}\right)^2 + \left(\frac{1}{2}\right)^2 = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
	$$
	Thus, the minimum value of $f(x, y) = x^2 + y^2$ subject to $x + y = 1$ is $\frac{1}{2}$ at the point $\left(\frac{1}{2}, \frac{1}{2}\right)$.
	
	\subsection{Inequality Constrained Optimization}
	Equality constrained optimization problems can be expressed as:
	$$
	\text{Minimize } f(\mathbf{x})
	\quad
	\text{subject to}
	\quad
	g_i(\mathbf{x}) \geq 0
	\quad
	\text{for } i = 1, \dots, m
	$$
	
	\subsubsection{Direct Method}
	When the constraint is active (i.e., it holds as an equality), it can be treated similarly to the equality-constrained case.
	If inactive, it is ignored.
	
	\textbf{Example:}
	Minimizing a Quadratic Function with an Inequality Constraint
	\\
	Minimize $f(x, y) = x^2 + y^2$ subject to $x \geq 0$ and $y \geq 0$.
	\\
	This can be approached by checking the boundary and interior points.
	The minimum will be at $(0,0)$ given the non-negative constraints.
	\\
	Note that if the constraint was $x+y \geq 1$, showing that the minimum is at $(0.5, 0.5)$ is much harder via direct calculations.
	
	\subsubsection{Lagrange Multiplier Method with Slack Variables}
	We note that:
	$$
	g_i(\mathbf{x}) \geq 0
	\iff
	g_i(\mathbf{x}) -s^2 = 0
	\quad
	\text{ for some } s \in \R
	$$
	Here, $s$ is called a "slack variable".
	It is then possible to use the Lagrange trick as before.
	\\
	The Lagrange multiplier method introduces now multiple auxiliary variables: the Lagrange multiplier and some slack variables to incorporate the inequality constraints into the objective function.
	
	\textbf{Lagrange Function:}
	$$
	\mathcal{L}(\mathbf{x}, \lambda)
	= f(\mathbf{x}) + \sum_{i=1}^m \lambda_i \left( g_i(\mathbf{x}) - s_i^2 \right)
	$$
	To find the stationary points, we solve the system:
	$$
	\nabla_\mathbf{x} \mathcal{L}(\mathbf{x}, \lambda) = \mathbf{0}
	$$
	
	\textbf{Example:}
	Minimizing a Quadratic Function with an Inequality Constraint\\
	Consider the problem of minimizing the function $f(x, y) = x^2 + y^2$ subject to the inequality constraint $x + y \geq 1$.
	
	\textbf{Step 1: Introduce the Slack Variable}\\
	The constraint can be written as:
	$$
	x + y - s^2 = 1 \quad \text{as } s^2 \geq 0 \quad \forall s \in \mathbb{R}
	$$
	where $s$ is the slack variable.
	The constraint is now an equality constraint.
	
	\textbf{Step 2: Construct the Lagrange Function}\\
	The Lagrange function incorporating the constraint is:
	$$
	\mathcal{L}(x, y, s, \lambda) = x^2 + y^2 + \lambda \left( (x + y) - s^2 - 1 \right)
	$$
	where $\lambda$ is the Lagrange multiplier.
	
	\textbf{Step 3: Compute the Partial Derivatives}\\
	To find the stationary points, take the partial derivatives of $\mathcal{L}$ with respect to $x$, $y$, $s$, and $\lambda$, and set them to zero:
	$$
	\frac{\partial \mathcal{L}}{\partial x} = 2x + \lambda = 0
	$$
	$$
	\frac{\partial \mathcal{L}}{\partial y} = 2y + \lambda = 0
	$$
	$$
	\frac{\partial \mathcal{L}}{\partial s} = -2\lambda s = 0
	$$
	$$
	\frac{\partial \mathcal{L}}{\partial \lambda} = x + y - s^2 - 1 = 0
	$$
	
	\textbf{Step 4: Solve the System of Equations}\\
	From $\frac{\partial \mathcal{L}}{\partial x} = 0$ and $\frac{\partial \mathcal{L}}{\partial y} = 0$, we have:
	$$
	2x + \lambda = 0 \quad \text{and} \quad 2y + \lambda = 0
	$$
	This implies $x = y$.\\
	From $\frac{\partial \mathcal{L}}{\partial s} = 0$, we have:
	$$
	-2\lambda s = 0
	$$
	This implies $\lambda = 0$ or $s = 0$. 
	
	\begin{enumerate}
		\item If $\lambda = 0$, then $x = 0$ and $y = 0$, but this would violate the constraint $x + y \geq 1$.
		Hence, this case is not valid.
		\item If $s = 0$, the constraint simplifies to $x + y = 1$.
		Since $x = y$, we have $2x = 1$, so $x = \frac{1}{2}$ and $y = \frac{1}{2}$.
	\end{enumerate}
	
	\textbf{Step 5: Determine the Optimal Point}\\
	Substitute $x = \frac{1}{2}$, $y = \frac{1}{2}$ into the original function:
	$f\left(\frac{1}{2}, \frac{1}{2}\right) = \frac{1}{2}$.
	Thus, the minimum value of $f(x, y) = x^2 + y^2$ subject to $x + y \geq 1$ is $\frac{1}{2}$ at the point $\left(\frac{1}{2}, \frac{1}{2}\right)$.
	
\end{document}
